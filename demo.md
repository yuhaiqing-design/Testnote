一、上海人工智能实验室青年科学家陈恺老师为我们带来《书生·浦语大模型全链路开源体系》的课程学习笔记
视频地址：书生·浦语大模型全链路开源体系_哔哩哔哩_bilibili

## 引言

- **大模型的重要性**：大模型被视为实现通用人工智能的关键途径，从专用模型向通用模型的转变标志着AI技术的进步。

- **书生浦语大模型**：代表了中国在大模型领域的开源努力和技术创新。



## 大模型发展趋势

- **从专用到通用**：早期研究集中在专用模型上，针对特定任务设计。近年来，研究转向通用大模型，一个模型解决多种任务。

- **跨模态能力**：通用大模型不仅能处理文本任务，还能跨模态处理图像、语音等，如GPT系列模型。



## 书生浦语大模型开源历程

- **快速迭代**：自首次发布以来，书生浦语大模型不断升级，支持更多语言和模态，提供不同规模的模型以适应不同需求。

- **全链条开源**：从数据集到预训练、微调、部署和评测，书生浦语大模型提供了一套完整的开源工具链。



## 模型亮点

- **多尺寸模型**：提供7B轻量级和20B重量级模型，满足不同性能和成本需求。

- **高质量语料**：通过新一代数据清洗技术，提升语料质量，加强模型的语言建模能力。

- **性能提升**：新一代模型在多个评测集上展现出色性能，特别是在中文场景下具有优势。



## 工具体系详述

- **数据集**：书生万卷1.0和书生外传CC提供了丰富的中文语料，支持模型迭代。

- **预训练框架**：Intel Evo支持大规模训练，提高训练效率和兼容性。

- **微调框架**：X Tuner支持多种微调策略，适配不同硬件和生态。

- **评测体系**：Open Compass 4南提供了全面的评测工具链，促进社区共建评测基准。

- **部署解决方案**：AM Deploy提供模型轻量化和高效推理引擎，优化模型上线性能。



## 应用实践与思考

- **模型选型**：根据业务需求选择合适的模型尺寸和类型，考虑算力和成本。

- **微调策略**：根据业务复杂性选择全参数或部分参数微调，注入领域知识。

- **评测与部署**：通过Open Compass 4南评测模型性能，使用AM Deploy优化部署。



## 总结与展望

- **开源赋能**：书生浦语大模型的开源战略有助于推动AI技术的创新和普及。

- **社区贡献**：通过开源工具链，书生浦语大模型鼓励社区参与，共同推动大模型技术发展。



## 个人思考

- **技术创新**：书生浦语大模型的快速迭代和技术创新展示了中国在AI领域的进步。

- **应用前景**：多模态能力和跨领域应用潜力使得大模型在未来AI应用中扮演关键角色。

- **社区共建**：开源体系的建立有助于形成活跃的技术社区，促进知识共享和技术交流。

- **持续发展**：随着技术的不断进步，期待书生浦语大模型在未来能够解决更多复杂问题，提供更丰富的应用场景。

二、InternLM2 技术报告笔记
链接：https://arxiv.org/pdf/2403.17297.pdf

本文介绍了一种名为InternLM2的开源大型语言模型，它在综合评估、长上下文建模和开放性主观评估等方面表现优异。该模型采用了创新的预训练和优化技术，并详细介绍了其预训练过程中的数据准备方式。通过使用不同的训练阶段和模型大小，作者提供了对模型演化的见解。此外，该模型还采用了一些新颖的技术，如监督微调和条件在线强化学习等，以提高其性能并解决人类偏好冲突的问题。

论文方法
方法描述
本文介绍了InternEvo框架，该框架可以高效地训练大规模预训练模型，并支持多种并行化策略。InternEvo采用了数据、序列、管道和混合精度等多维度并行化技术来优化GPU内存使用效率。此外，为了提高通信效率，InternEvo还引入了自适应划分技术和异步通信机制。在训练过程中，InternEvo还可以根据实际情况动态调整参数设置，以进一步提升训练效果。

方法改进
与传统的单机训练相比，InternEvo采用了分布式训练的方式，能够充分利用多个GPU的计算资源，从而加速训练过程。同时，InternEvo还支持多种并行化策略，包括数据、序列、管道和混合精度等，可以根据不同的任务需求选择合适的策略来进行训练。此外，InternEvo还引入了自适应划分技术和异步通信机制，可以在保证通信效率的同时，最大程度地利用GPU的计算能力。

解决的问题
通过使用InternEvo框架，我们可以更加高效地训练大规模预训练模型，特别是针对需要处理大量数据的任务，如自然语言处理、计算机视觉等领域。此外，InternEvo还支持多种并行化策略，可以根据不同的任务需求选择合适的策略来进行训练，从而更好地满足实际应用的需求。






论文实验
本文主要介绍了大规模预训练语言模型（LLM）的性能和局限性，并对InternLM2进行了详细的实验分析。文章分为两个部分，第一部分是关于InternLM2在多个领域的性能表现，第二部分是对数据污染问题的探讨。

在第一部分中，作者使用了多种实验方法来评估InternLM2在不同任务上的性能。首先，他们通过下采样技术将人类知识库中的数据集划分为低资源和高资源两组，然后比较了模型在这两组数据上的性能差异。结果表明，在低资源情况下，InternLM2相对于其他模型具有更好的泛化能力。其次，作者还对InternLM2在问答、阅读理解等常见NLP任务上的性能进行了测试，并与其他模型进行了比较。结果显示，InternLM2在这些任务上表现出色，特别是在需要处理长文本的情况下。此外，作者还研究了InternLM2在多语言环境下的性能，并发现它能够有效地适应不同的语言。

然而，文章也指出了InternLM2存在的一些局限性。例如，当面对一些复杂的推理问题时，InternLM2的表现并不理想。此外，文章还指出，由于数据集中可能存在偏差或错误，因此模型可能会受到误导，导致其生成不准确的答案。为了解决这个问题，作者提出了一种新的算法，可以自动检测并纠正数据集中的错误信息。

总的来说，本文通过对InternLM2的全面实验分析，揭示了其优点和局限性，为我们更好地理解和应用这种新型语言模型提供了有价值的参考。




论文总结
文章优点
本文介绍了InternLM2大型语言模型，并对其进行了详细的训练过程描述以及数据准备指导。该模型在各种主观和客观评估中表现出色，展示了强大的情感和问题解决能力。此外，文章还详细阐述了如何有效处理预训练数据，以及如何扩展模型的上下文长度，这些内容对于提高模型性能非常有帮助。最后，作者提出了COOL RLHF等创新的强化学习技术，进一步提高了模型的表现。

方法创新点
本文的方法创新主要体现在以下几个方面：

数据准备：作者详细介绍了如何准备预训练数据、增强数据、监督微调数据和人类反馈数据，为其他研究人员提供了有用的参考。
上下文长度扩展：作者采用了Group Query Attention（GQA）技术来减少推理成本，在预训练阶段先使用4k文本进行训练，然后将训练语料库转换为高质量的32k文本以进一步训练。通过这种方法，模型能够更好地处理长上下文信息。
强化学习：作者提出了COOL RLHF技术，这是一种条件在线强化学习技术，可以协调不同但可能相互冲突的偏好，并在多个回合内执行近似策略梯度优化，从而避免奖励黑客行为。
未来展望
本文提出的InternLM2模型已经在各个评估指标上取得了优异的成绩，但仍有一些改进的空间。例如，可以通过更深入地研究预训练数据的质量和多样性来进一步提高模型的表现。此外，可以探索更多的数据增强技术和上下文长度扩展方法，以便更好地满足各种应用场景的需求。总之，本文的研究成果为大规模语言模型的发展提供了一个重要的基础，未来的研究将继续在这个方向上努力。
